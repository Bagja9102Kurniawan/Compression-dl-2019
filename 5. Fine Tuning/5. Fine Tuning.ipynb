{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 1. Fine Tuning.ipynb","provenance":[{"file_id":"1IW5_GK79SQv80kWgVKvocgkybWMLpaUm","timestamp":1572423996184}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PHBSJxyi2v2v","colab_type":"text"},"source":["![alt text](https://anvaqta.id/headerai.jpg)"]},{"cell_type":"markdown","metadata":{"id":"_Cv-ECYwgO6D","colab_type":"text"},"source":["# Fine Tuning CNN"]},{"cell_type":"markdown","metadata":{"id":"q2znBWbxiTfi","colab_type":"text"},"source":["---\n","## Kenapa Perlu Fine Tuning?\n","Fine tuning sangat penting untuk meningkatkan akurasi model dan juga mengeneralisasi data input. Fine tuning juga berguna untuk menghindari overfitting"]},{"cell_type":"markdown","metadata":{"id":"EHWuGde508se","colab_type":"text"},"source":["---\n","## Dataset\n","Dataset pun juga perlu dilakukan preproses agar akurasi model menjadi lebih baik. beberapa metode Normalisasi dataset yaitu\n","\n","\n","\n","1.   Zero-Center\n","2.   Range [0,1]\n","3.   Zero Center dengan range [-1,1]\n"]},{"cell_type":"markdown","metadata":{"id":"HwTuBH9dpABb","colab_type":"text"},"source":["---\n","## Fungsi Aktivasi\n","\n"]},{"cell_type":"markdown","metadata":{"id":"10qTY9eOpxbe","colab_type":"text"},"source":["---\n","### Sigmoid\n","\n","fungsi sigmoid memiliki rentang output antara [0, 1]. Fungsi sigmoid cocok digunakan sebagai layer output untuk binary classifier\n","\n","![alt text](https://miro.medium.com/max/4000/1*JHWL_71qml0kP_Imyx4zBg.png)"]},{"cell_type":"markdown","metadata":{"id":"RFO6TMr6p5yb","colab_type":"text"},"source":["---\n","### ReLU\n","\n","fungsi ReLU cocok digunakan untuk fungsi aktivasi karena biasanya akan menghasilkan akurasi yang tinggi\n","\n","![alt text](https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png)"]},{"cell_type":"markdown","metadata":{"id":"iz4v4vvgqxGs","colab_type":"text"},"source":["---\n","### Tanh\n","\n","Tanh mirip seperti sigmoid dan lebih cepat dari sigmoid karena tidak adanya perhitungan eksponensial\n","\n","![alt text](data:image/png;base64,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)\n","\n","![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1280px-Hyperbolic_Tangent.svg.png)"]},{"cell_type":"markdown","metadata":{"id":"OQxD-jWbrsEb","colab_type":"text"},"source":["---\n","### Softmax\n","\n","Softmax merupakan sebuah fungsi aktivasi yang digunakan untuk menghitung loss dari model yang memiliki lebih dari 1 kelas, biasanya digunakan untuk output yang one hot matrix\n","\n","![alt text](https://ljvmiranda921.github.io/assets/png/cs231n-ann/softmax.png)"]},{"cell_type":"markdown","metadata":{"id":"ABVQ6_l3piKY","colab_type":"text"},"source":["---\n","## Loss Function\n","\n","\n","1.   Mean Squared Error => Cocok untuk data regresi\n","2.   Categorical Crossentropy => Cocok untuk data yang memiliki banyak label\n","3.   Binary Crossentropy => Cocok untuk data biner\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nBKb9WfwsNVB","colab_type":"text"},"source":["---\n","## Learning Rate & Decay\n","\n","**Learning rate** adalah hyperparameter yang bertugas mengurangi gradient yang akan ditambahkan kebobot dengan tujuan memperlambat proses learning sehingga diharapkan bisa mencapai global optimum\n","\n","**Decay** merupakan sebuah hyperparameter yang bertujuan untuk mengurangi nilai dari learning rate setiap **n** epochs"]},{"cell_type":"markdown","metadata":{"id":"CupOEP3QizXL","colab_type":"text"},"source":["---\n","# Let's Code\n","Seperti biasa, install `tensorflow 2`"]},{"cell_type":"code","metadata":{"id":"pu4GI-qdgKUq","colab_type":"code","colab":{}},"source":["!pip install tensorflow-gpu==2.0.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpNSGni1gRg-","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGPBParlkCT8","colab_type":"text"},"source":["Import library yang diperlukan"]},{"cell_type":"code","metadata":{"id":"8kazrXP5gXHD","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1kXOmS6g4GM","colab_type":"text"},"source":["---\n","## Load Dataset"]},{"cell_type":"code","metadata":{"id":"En8bHYt4g25s","colab_type":"code","colab":{}},"source":["(X_train_raw, y_train), (X_test_raw, y_test) = cifar10.load_data()\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","class_num = len(class_names)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMQlRHWPhZJM","colab_type":"text"},"source":["---\n","## Prepocessing\n","Normalisasi nilai piksel menjadi antara 0 dan 1"]},{"cell_type":"code","metadata":{"id":"jGEXtZmLu_m-","colab_type":"code","colab":{}},"source":["# Bagi X_train_raw dan X_test_raw dengan 255.0\n","X_train = X_train_raw/255.0\n","X_test = X_test_raw/255.0\n","\n","print('x_train shape:', X_train.shape)\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOChYgoNki5J","colab_type":"text"},"source":["---\n","## Visualisasi Dataset"]},{"cell_type":"code","metadata":{"id":"nOEPtuqpkmKL","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(X_train_raw[i], cmap=plt.cm.binary)\n","    # The CIFAR labels happen to be arrays, \n","    # which is why you need the extra index\n","    plt.xlabel(class_names[y_train[i][0]])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Bc5EkjJlzZk","colab_type":"text"},"source":["---\n","## One Hot Matriks Label"]},{"cell_type":"code","metadata":{"id":"7RPknsgTl69d","colab_type":"code","colab":{}},"source":["# gunakan fungsi to_categorical untuk membuat one hot matriks\n","y_train_hot = to_categorical(y_train, class_num)\n","y_test_hot = to_categorical(y_test, class_num)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8v84QFanaa7","colab_type":"text"},"source":["---\n","## Bagi Data Validasi dan Data Train"]},{"cell_type":"code","metadata":{"id":"ahtRazbIneEK","colab_type":"code","colab":{}},"source":["X_val = X_train[-10000:,:]\n","y_val = y_train_hot[-10000:]\n","\n","X_train = X_train[:-10000, :]\n","y_train_hot = y_train_hot[:-10000]\n","\n","print('X_val.shape   =',X_val.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('X_train.shape =',X_train.shape)\n","print('y_train.shape =',y_train_hot.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E610ZukdgvUU","colab_type":"text"},"source":["## Membuat Model CNN"]},{"cell_type":"code","metadata":{"id":"6ikqzGjvgudX","colab_type":"code","colab":{}},"source":["def buat_model():\n","  model = Sequential([\n","        # Buat layer Conv2D dengan 64 filter berukuran 3x3, input_shape (32,32,3) dang fungsi aktivasi relu\n","        Conv2D(64, (3,3), input_shape=(32,32,3), activation='relu'),\n","        # Buat layer MaxPooling2D dengan ukuran filter 2x2\n","        MaxPooling2D((2,2)),\n","        # Buat layer Conv2D dengan 64 filter berukuran 3x3 dan fungsi aktivasi relu\n","        Conv2D(64, (3,3), activation='relu'),\n","        # Buat layer Maxpoolin2D dengan ukuran filter 2x2\n","        MaxPooling2D((2,2)),\n","        # Buat layer Flatten\n","        Flatten(),\n","        # Buat layer Dense dengan 64 neuron dan fungsi aktivasi relu\n","        Dense(64, activation='relu'),\n","        # Buat layer output Dense dengan 10 neuron dan fungsi aktivasi softmax\n","        Dense(class_num, activation='softmax')\n","  ])\n","  return model\n","model = buat_model()\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMltyCKemoN6","colab_type":"text"},"source":["---\n","## Inisialisasi Hyperparameter"]},{"cell_type":"code","metadata":{"id":"F74RedDsmqor","colab_type":"code","colab":{}},"source":["num_epochs = 10\n","batch_size = 1000\n","history = {}\n","scores = {}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QkYkjkS7uw-","colab_type":"text"},"source":["---\n","# Optimizers API\n","\n","Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. \n","\n","At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent.\n","\n","<br>\n","\n","\n","<center>\n","<img src='http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif' height=300><br><img src='https://image.ibb.co/gHFUTz/opt2.gif' height=250>\n","<img src='https://miro.medium.com/max/1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif' height=250>\n","  \n","  \n","</center>\n","\n","<br>\n","\n","\n","For the next vanilla SGD, we give you the implementation as you've implement it many times before.\n","\n","But now we write it as a function, so <font color='red'>**Observe carefully**</font> how the implementation is done. \n","\n","Then complete the next exercises with similar implementation"]},{"cell_type":"markdown","metadata":{"id":"V2t8s5gU7uw_","colab_type":"text"},"source":["---\n","## Stochastic Gradient Descent\n","\n","The most basic optimization in Neural Network\n","<center>\n","<img src='https://image.ibb.co/em1Q1K/sgd.png' width=700>\n","</center>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        perform gradient update\n","      </font></td>\n","    <td>:</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_{t+1}=W_t - \\alpha\\nabla f(W_t)\n","      \\end{align}\n","      $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\alpha\\\\W_t\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(lr) learning rate<br><br>(w)  old weight</pre>  </font>    \n","    </td>    \n","    <td>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iUOrv9nEi3Y_","colab_type":"code","colab":{}},"source":["# Buat optimizer SGD dengan learning rate 0.01 dan decay 1e-6\n","sgd = SGD(learning_rate=0.01, decay=1e-6)\n","model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n","history['sgd'] = model.fit(X_train, y_train_hot,\n","                           validation_data=(X_val, y_val),\n","                           epochs=num_epochs,\n","                           batch_size=batch_size,\n","                           verbose=2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sK8RDBS8r5jk","colab_type":"text"},"source":["---\n","## Hitung Score Model SGD"]},{"cell_type":"code","metadata":{"id":"mHrA0EeFr-Q4","colab_type":"code","colab":{}},"source":["scores['sgd'] = model.evaluate(X_test, y_test_hot, verbose=1)\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores['sgd'][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KANu9uLE7uxj","colab_type":"text"},"source":["---\n","## Root Mean Square Propagation (RMSprop)\n","\n","RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in [Lecture 6e of his Coursera Class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n","\n","It’s famous for not being published, yet being very well-known; most deep learning framework include the implementation of it out of the box.\n","\n","RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"30qv8phce-3B"},"source":["---\n","The basic formula for RMSProp Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        calculate new decayed<br> gradient build-up<br><br><br>\n","        perform rmsprop update\n","      </font></td>\n","    <td>:<br><br><br><br>:</td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        \\\\g_t &= \\gamma\\ g_{t-1} + (1-\\gamma)\\ \\nabla f(W_t)^2\\\\\\\\\n","        W_{t+1}&=W_t -\\frac{\\alpha}{\\sqrt{g_t}+\\epsilon}\\nabla f(W_t)\\\\\n","        \\end{align}\n","        $$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      g_t\\\\\\gamma\\\\\\alpha\\\\\\epsilon\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(gt)  gradient build-up<br><br>(dr)  decay rate<br><br>(lr)  learning rate<br><br>(e)   epsilon </pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"wua4bRK-jcQN","colab_type":"code","colab":{}},"source":["# Buat optimizer rmsprop dengan learning_rate 0.001\n","rmsprop = RMSprop(learning_rate=0.001)\n","model = buat_model()\n","model.compile(optimizer=rmsprop, loss='categorical_crossentropy', metrics=['accuracy'])\n","history['rmsprop'] = model.fit(X_train, y_train_hot,\n","                           validation_data=(X_val, y_val),\n","                           epochs=num_epochs,\n","                           batch_size=batch_size,\n","                           verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2OLbhb_sScu","colab_type":"text"},"source":["---\n","## Hitung Score Model RMSprop"]},{"cell_type":"code","metadata":{"id":"APSXaNKlsZwm","colab_type":"code","colab":{}},"source":["scores['rmsprop'] = model.evaluate(X_test, y_test_hot, verbose=1)\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores['rmsprop'][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ysUITZiu7uxs","colab_type":"text"},"source":["---\n","## Adaptive Moment Estimation Optimizer (Adam)\n","\n","ADAM is another method that computes adaptive learning rates for each parameter. \n","\n","In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum.\n","\n","Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. \n","\n","\n","[Kingma, D. P., & Ba, J. L. (2015). Adam: a Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rBvy85FpWyt7"},"source":["---\n","The basic formula for ADAM Update is as follow:\n","\n","\n","<br>\n","\n","<table> \n","  <tr>\n","    <td>\n","      <font size=2>\n","        first moment (mean)<br> build-up<br><br><br>\n","        second moment (variance)<br>build-up<br><br><br><br>\n","        bias correction<br><br><br><br><br>\n","        perform adam update<br><br>\n","      </font></td>\n","    <td>:<br><br><br><br><br>:<br><br><br><br><br>:<br><br><br><br><br>:<br><br></td>\n","    <td>\n","      <font size=3>\n","        $$\n","        \\begin{align}\n","        m_t &= \\beta_1\\ m_{t-1} + (1-\\beta_1)\\ \\nabla f(W_t)\\\\\\\\\n","        v_t &= \\beta_2\\ v_{t-1} + (1-\\beta_2)\\ \\nabla f(W_t)^2\\\\\\\\\n","        \\hat{m_t}&=\\frac{m_t}{1-(\\beta_1)^t} \\ \\ ;\\ \\ \n","        \\hat{v_t} =\\frac{v_t}{1-(\\beta_2)^t}\\\\\\\\\n","        W_{t+1}&=W_t -\\frac{\\alpha}{\\sqrt{\\hat{v_t}}+\\epsilon}\\hat{m_t}\n","        \\end{align}\n","$$</font>\n","    </td>\n","  </tr>\n","</table>\n","\n","<br>\n","\n","\n","<table> \n","  <tr>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\beta_1\\\\\\beta_2\\\\m_t\\\\v_t\\\\\\hat{m_t}\\\\\\hat{v_t}\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(beta1) beta mean<br><br>(beta2) beta variance<br><br>(m)     1st moment (mean) buildup<br><br>(v)     2nd moment (variance) buildup<br><br>(mb)    1st moment (mean) bias<br><br>(mv)    2nd moment (variance) bias</pre>  </font>    \n","    </td>\n","    <td>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|<br>|</td>\n","    <td><font size=3>\n","      $$\n","      \\begin{align}\n","      \\alpha\\\\\\epsilon\\\\W_t\\\\W_{t+1}\\\\\\nabla f(W_t)\n","      \\end{align}\n","      $$ </font>\n","    </td>\n","    <td><font size=2>\n","      <pre>(lr)    learning rate<br><br>(e)     epsilon <br><br>(w)     old weight<br><br>(new_w) new weight<br><br>(grad)  weight gradient </pre>  </font>    \n","    </td>\n","  </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"8QezxuHArtX-","colab_type":"code","colab":{}},"source":["# Buat optimizer adam dengan learning_rate 0.001, beta_1=0.9, beta_2=0.999\n","adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n","model = buat_model()\n","model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n","history['adam'] = model.fit(X_train, y_train_hot,\n","                           validation_data=(X_val, y_val),\n","                           epochs=num_epochs,\n","                           batch_size=batch_size,\n","                           verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0YncVVdsdiG","colab_type":"text"},"source":["---\n","## Hitung Score Model Adam"]},{"cell_type":"code","metadata":{"id":"s7sHF9Vesf59","colab_type":"code","colab":{}},"source":["scores['adam'] = model.evaluate(X_test, y_test_hot, verbose=1)\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores['adam'][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dChwIkVnsiOH","colab_type":"text"},"source":["---\n","## Perbandingan Loss dan Akurasi Tiap Optimizer"]},{"cell_type":"code","metadata":{"id":"GIyNLxRqsm9C","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1,2,figsize=(18,3))\n","\n","ax[0].plot(history['sgd'].history['loss'])\n","ax[0].plot(history['rmsprop'].history['loss'])\n","ax[0].plot(history['adam'].history['loss'])\n","ax[0].set_title('Train Loss')\n","ax[0].set_ylabel('Loss')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['SGD', 'RMSprop', 'Adam'], loc='upper right')\n","\n","ax[1].plot(history['sgd'].history['accuracy'])\n","ax[1].plot(history['rmsprop'].history['accuracy'])\n","ax[1].plot(history['adam'].history['accuracy'])\n","ax[1].set_title('Train Accuracy')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['SGD', 'RMSprop', 'Adam'], loc='upper right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V37SG8xhvZJW","colab_type":"text"},"source":["---\n","## Perbandingan Akurasi"]},{"cell_type":"code","metadata":{"id":"mtlBbD56vayc","colab_type":"code","colab":{}},"source":["print('SGD Accuracy: %.2f%%' % (scores['sgd'][1]*100))\n","print('RMSprop Accuracy: %.2f%%' % (scores['rmsprop'][1]*100))\n","print('Adam Accuracy: %.2f%%' % (scores['adam'][1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjoKQI0B7uwq","colab_type":"text"},"source":["---\n","# Dropout Layer\n","\n","Dropout [1] is a technique for regularizing neural networks by **randomly** setting some features to **zero** during the forward pass of training. \n","\n","<center>\n","<img src=\"https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=500>\n"," </center>\n","\n","In this exercise you will implement a dropout layer and modify your fully-connected network to optionally use dropout.\n","\n","[1] [Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012](https://arxiv.org/abs/1207.0580)"]},{"cell_type":"markdown","metadata":{"id":"_cTZY5Mybksv","colab_type":"text"},"source":["---\n","## Mari Buat Modelnya"]},{"cell_type":"markdown","metadata":{"id":"9BXWazi6k92t","colab_type":"text"},"source":["---\n","### Tanpa Dropout"]},{"cell_type":"code","metadata":{"id":"lMGyEGU9Xnft","colab_type":"code","colab":{}},"source":["def buat_model_tanpa_dropout():\n","  model = Sequential([\n","          # Buat layer Flatten dengan input_shape 32x32x3\n","          Flatten(input_shape=(32,32,3)),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat layer Dense dengan 10 neuron dan fungsi aktivasi softmax\n","          Dense(10, activation='softmax')\n","  ])\n","  return model\n","model_tanpa_dropout = buat_model_tanpa_dropout()\n","model_tanpa_dropout.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_MKCPYdlB3_","colab_type":"text"},"source":["---\n","### Train Modelnya"]},{"cell_type":"code","metadata":{"id":"ciRHAeBsk3V6","colab_type":"code","colab":{}},"source":["# set loss nya menggunakan categorical_crossentropy\n","model_tanpa_dropout.compile(optimizer='adam', loss=??, metrics=['accuracy'])\n","history_tanpa_dropout = model_tanpa_dropout.fit(X_train, y_train_hot,\n","                                                validation_data=(X_val, y_val),\n","                                                epochs=10,\n","                                                batch_size=1000,\n","                                                verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1l5uwA5lXC8","colab_type":"code","colab":{}},"source":["akurasi_tanpa_dropout = model_tanpa_dropout.evaluate(X_test, y_test_hot)\n","print('Akurasi : %.2f%%' % (akurasi_tanpa_dropout[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U1RNEAWGl3Hn","colab_type":"text"},"source":["---\n","### Dengan Dropout"]},{"cell_type":"code","metadata":{"id":"PbPbQnw6llw8","colab_type":"code","colab":{}},"source":["def buat_model_dropout():\n","  model = Sequential([\n","          # Buat layer Flatten dengan input_shape 32x32x3\n","          Flatten(input_shape=(32,32,3)),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat Dropout dengan probability 0.5\n","          Dropout(0.5),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat Dropout dengan probability 0.5\n","          Dropout(0.5),\n","          # Buat layer Dense dengan 500 neuron dan fungsi aktivasi relu\n","          Dense(500, activation='relu'),\n","          # Buat Dropout dengan probability 0.5\n","          Dropout(0.5),\n","          # Buat layer Dense dengan 10 neuron dan fungsi aktivasi softmax\n","          Dense(10, activation='softmax')\n","  ])\n","  return model\n","model_dropout = buat_model_dropout()\n","model_dropout.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMEPpORcmuLK","colab_type":"text"},"source":["---\n","### Train Modelnya"]},{"cell_type":"code","metadata":{"id":"FK1XwAZllyWU","colab_type":"code","colab":{}},"source":["model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","history_model_dropout = model_dropout.fit(X_train, y_train_hot,\n","                                          validation_data=(X_val, y_val),\n","                                          epochs=10,\n","                                          batch_size=1000,\n","                                          verbose=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5j2UHb49nP48","colab_type":"text"},"source":["---\n","### Hitung Evaluasinya"]},{"cell_type":"code","metadata":{"id":"o4k3yo1fnAPS","colab_type":"code","colab":{}},"source":["score_dropout = model_dropout.evaluate(X_test, y_test_hot)\n","print('Akurasi : %2.f%%' % (score_dropout[1]*100))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5PtU0X9InTy3","colab_type":"text"},"source":["---\n","## Bandingkan Hasilnya"]},{"cell_type":"code","metadata":{"id":"BmxQBcg_nOxf","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1,2,figsize=(18,3))\n","\n","ax[0].plot(history_tanpa_dropout.history['loss'])\n","ax[0].plot(history_model_dropout.history['loss'])\n","ax[0].set_title('Train Loss')\n","ax[0].set_ylabel('Loss')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['Tanpa Dropout', 'Dengan Dropout'], loc='upper right')\n","\n","ax[1].plot(history_tanpa_dropout.history['accuracy'])\n","ax[1].plot(history_model_dropout.history['accuracy'])\n","ax[1].set_title('Train Accuracy')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['Tanpa Dropout', 'Dengan Dropout'], loc='lower right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drGprDiQn5ny","colab_type":"text"},"source":["---\n","## Perbandingan Akurasi"]},{"cell_type":"code","metadata":{"id":"j9NWApeFntvE","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(1,2,figsize=(18,3))\n","\n","ax[0].plot(history_tanpa_dropout.history['accuracy'])\n","ax[0].plot(history_tanpa_dropout.history['val_accuracy'])\n","ax[0].set_title('Tanpa Dropout')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['Train Accuracy', 'Val Accuracy'], loc='lower right')\n","\n","ax[1].plot(history_model_dropout.history['accuracy'])\n","ax[1].plot(history_model_dropout.history['val_accuracy'])\n","ax[1].set_title('Dengan Dropout')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['Train Accuracy', 'Val Accuracy'], loc='lower right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rtqo6vIGsQ8i","colab_type":"text"},"source":["*copyright © 2019 Artificial intelligence laboratory all right reserved.*"]}]}